{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "from time import time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(sentences_array,file_output_name):\n",
    "    if os.path.isfile(file_output_name):\n",
    "        os.remove(file_output_name)\n",
    "    with codecs.open(file_output_name, \"a\",encoding='utf-8') as a_file:\n",
    "        for elem in sentences_array:\n",
    "            a_file.write(elem)\n",
    "            a_file.write(\"\\n\")\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_shuffled(file_name):\n",
    "    src_words=[]\n",
    "    tgt_words=[]\n",
    "    all_src_tgt=[]\n",
    "    with codecs.open(file_name, encoding='utf-8') as f:\n",
    "#         sentences_words=[]\n",
    "        for line in f.readlines():\n",
    "            line_array=line.strip().split('\\t')\n",
    "            all_src_tgt.append(line_array)\n",
    "        f.close()\n",
    "    \n",
    "    random.shuffle(all_src_tgt)\n",
    "    \n",
    "    for elem in all_src_tgt:\n",
    "        src_words.append(elem[0])\n",
    "        tgt_words.append(elem[1])\n",
    "        \n",
    "    return src_words,tgt_words,all_src_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=r\"C:\\Users\\Hanane\\Documents\\Python_Scripts\\TelecomParis\\Qwant\\dataset_wiki_NER_fr\"\n",
    "# filename=\"data_Wiki_NER_filtered.sent-ner\"\n",
    "# path+\"\\\\\"+filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path=r\"C:\\Users\\Hanane\\Documents\\Python_Scripts\\TelecomParis\\Qwant\\dataset_wiki_NER_fr\"\n",
    "src_words,tgt_words,all_src_tgt=get_sentences_shuffled(path+\"\\\\data_Wiki_NER_filtered.sent-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_txt(src_words,path+\"\\\\wiki_words_bitext.txt\")\n",
    "save_to_txt(tgt_words,path+\"\\\\wiki_tagss_bitext.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train valid, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split a part of the initial dataset on train, valid, test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "original file ==> 20M rows\n",
    "Work only on 6.25M rows\n",
    "train lines ==> 5M rows\n",
    "valid and test lines ==> 625K rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Be carefull before lunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed -n '1,5000000p' wiki_train_words_bitext_large_2.txt > wiki_train_words_bitext.txt\n",
    "sed -n '1,5000000p' wiki_train_tags_bitext_large_2.txt > wiki_train_tags_bitext.txt\n",
    "# path=\"./Qwant_opennmt_NER_french/dataset/train_valid\"\n",
    "# sed -n '1,5000000p' wiki_words_bitext.txt > $path+wiki_train_words_bitext.txt\n",
    "# sed -n '1,5000000p' wiki_tags_bitext.txt > $path+wiki_train_tags_bitext.txt\n",
    "\n",
    "sed -n '1,625000p' wiki_valid_words_bitext_large_2.txt > wiki_valid_words_bitext.txt\n",
    "sed -n '1,625000p' wiki_valid_tags_bitext_large_2.txt > wiki_valid_tags_bitext.txt\n",
    "# sed -n '5000001,5625000p' wiki_words_bitext.txt > $path+wiki_valid_words_bitext.txt\n",
    "# sed -n '5000001,5625000p' wiki_tags_bitext.txt > $path+wiki_valid_tags_bitext.txt\n",
    "\n",
    "\n",
    "sed -n '1,625000p' wiki_test_words_bitext_large_2.txt > wiki_test_words_bitext.txt\n",
    "sed -n '1,625000p' wiki_test_tags_bitext_large_2.txt > wiki_test_tags_bitext.txt\n",
    "# sed -n '5625000,6250000p' wiki_words_bitext.txt > $path+wiki_test_words_bitext.txt\n",
    "# sed -n '5625000,6250000p' wiki_tags_bitext.txt > $path+wiki_test_tags_bitext.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old method to split and save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work with all_src_tgt instead of zipping the whole data set ==> and try to get words and tags by slicing the arrays instead of comprehensive lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in zip\n",
      "zip 319.613s.\n",
      "shuffle 71.278s.\n",
      "build train-valid-test 1.522s.\n",
      "build valid 509.012s.\n",
      "build test 585.912s.\n",
      "build train 3141.623s.\n"
     ]
    }
   ],
   "source": [
    "print(\"in zip\")\n",
    "t0=time()\n",
    "whole_dataset=list(zip(src_words,tgt_words))\n",
    "print(\"zip %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "    \n",
    "t0=time()\n",
    "random.shuffle(whole_dataset)\n",
    "print(\"shuffle %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "t0=time()\n",
    "train_size=int(0.8*len(whole_dataset))\n",
    "valid_size=int(0.9*len(whole_dataset))\n",
    "train_set=whole_dataset[:train_size]\n",
    "valid_set=whole_dataset[train_size:valid_size]\n",
    "test_set=whole_dataset[valid_size:]\n",
    "print(\"build train-valid-test %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "\n",
    "t0=time()\n",
    "valid_words=[elem[0] for elem in valid_set]\n",
    "valid_tags=[elem[1] for elem in valid_set]\n",
    "print(\"build valid %0.3fs.\" % (time() - t0),,\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "t0=time()\n",
    "test_words=[elem[0] for elem in test_set]\n",
    "test_tags=[elem[1] for elem in test_set]\n",
    "print(\"build test %0.2fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "t0=time()\n",
    "train_words=[elem[0] for elem in train_set]\n",
    "train_tags=[elem[1] for elem in train_set]\n",
    "print(\"build train %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O O O O O O B-LOC_CITY',\n",
       " 'B-DATE I-DATE I-DATE O B-LOC_CITY',\n",
       " 'O O O O B-MISC I-MISC O',\n",
       " 'B-DATE O O O O O',\n",
       " 'I-PEOPLE O B-PEOPLE I-PEOPLE',\n",
       " 'O O B-LOC_COUNTRY O',\n",
       " 'O O O O O O B-DATE O',\n",
       " 'O O O O O O O O B-MISC',\n",
       " 'O O O O O O B-LOC_CITY O',\n",
       " 'O O B-PEOPLE I-PEOPLE O O O O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_to_txt(train_words,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_train_words_bitext_large_2.txt\")\n",
    "save_to_txt(train_tags,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_train_tags_bitext_large_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_to_txt(valid_words,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_valid_words_bitext_large_2.txt\")\n",
    "save_to_txt(valid_tags,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_valid_tags_bitext_large_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_to_txt(test_words,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_test_words_bitext_large_2.txt\")\n",
    "save_to_txt(test_tags,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_test_tags_bitext_large_2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vocabularies ==> word, tag and tkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-05 18:12:12.681953: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-08-05 18:12:12.683149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-08-05 18:12:23.161844: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2020-08-05 18:12:23.162844: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-05 18:12:23.173491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-79FP3JFQ\n",
      "2020-08-05 18:12:23.174533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-79FP3JFQ\n",
      "2020-08-05 18:12:23.185612: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#254294 rows\n",
    "# path=\"../dataset/train_valid/\"\n",
    "!onmt-build-vocab --size 1000000 --save_vocab ../dataset/train_valid/wiki-src-train-vocab.txt ../dataset/train_valid/wiki_train_words_bitext.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-05 18:14:23.636003: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-08-05 18:14:23.636828: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-08-05 18:14:28.064471: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2020-08-05 18:14:28.065013: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-05 18:14:28.073112: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-79FP3JFQ\n",
      "2020-08-05 18:14:28.073844: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-79FP3JFQ\n",
      "2020-08-05 18:14:28.074553: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!onmt-build-vocab --size 1000 --save_vocab ../dataset/train_valid/wiki-tgt-train-vocab.txt ../dataset/train_valid/wiki_train_tags_bitext.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-05 18:16:00.109992: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-08-05 18:16:00.110700: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-08-05 18:16:04.364493: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2020-08-05 18:16:04.365058: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-05 18:16:04.373495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-79FP3JFQ\n",
      "2020-08-05 18:16:04.374255: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-79FP3JFQ\n",
      "2020-08-05 18:16:04.374995: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!onmt-build-vocab --tokenizer_config ../config/char_tokenization.yml --size 10000 --save_vocab ../dataset/train_valid/wiki-src-train-tkt-vocab.txt ../dataset/train_valid/wiki_train_words_bitext.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sentences_from_src_tgt(file_name):\n",
    "#     src_words=[]\n",
    "#     tgt_words=[]\n",
    "#     with codecs.open(file_name, encoding='utf-8') as f:\n",
    "# #         sentences_words=[]\n",
    "#         for line in f.readlines():\n",
    "#             line_array=line.strip().split('\\t')\n",
    "# #             print(line_array[0])\n",
    "#             if len(line_array)>1:\n",
    "#                 src_words.append(line_array[0])\n",
    "#                 tgt_words.append(line_array[1])\n",
    "#         f.close()\n",
    "#     return src_words,tgt_words\n",
    "\n",
    "# def get_sentences_from_all(file_name):\n",
    "#     src_words=[]\n",
    "#     tgt_words=[]\n",
    "#     all_src_tgt=[]\n",
    "#     with codecs.open(file_name, encoding='utf-8') as f:\n",
    "# #         sentences_words=[]\n",
    "#         for line in f.readlines():\n",
    "#             line_array=line.strip().split('\\t')\n",
    "#             all_src_tgt.append(line_array)\n",
    "# #             print(line_array[0])\n",
    "#             if len(line_array)>1:\n",
    "#                 src_words.append(line_array[0])\n",
    "#                 tgt_words.append(line_array[1])\n",
    "#         f.close()\n",
    "#     return src_words,tgt_words,all_src_tgt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
