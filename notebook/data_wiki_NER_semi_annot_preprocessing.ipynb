{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_src_tgt(file_name):\n",
    "    src_words=[]\n",
    "    tgt_words=[]\n",
    "    with codecs.open(file_name, encoding='utf-8') as f:\n",
    "#         sentences_words=[]\n",
    "        for line in f.readlines():\n",
    "            line_array=line.strip().split('\\t')\n",
    "#             print(line_array[0])\n",
    "            if len(line_array)>1:\n",
    "                src_words.append(line_array[0])\n",
    "                tgt_words.append(line_array[1])\n",
    "        f.close()\n",
    "    return src_words,tgt_words\n",
    "\n",
    "def save_to_txt(sentences_array,file_output_name):\n",
    "    if os.path.isfile(file_output_name):\n",
    "        os.remove(file_output_name)\n",
    "    with codecs.open(file_output_name, \"a\",encoding='utf-8') as a_file:\n",
    "        for elem in sentences_array:\n",
    "            a_file.write(elem)\n",
    "            a_file.write(\"\\n\")\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_all(file_name):\n",
    "    src_words=[]\n",
    "    tgt_words=[]\n",
    "    all_src_tgt=[]\n",
    "    with codecs.open(file_name, encoding='utf-8') as f:\n",
    "#         sentences_words=[]\n",
    "        for line in f.readlines():\n",
    "            line_array=line.strip().split('\\t')\n",
    "            all_src_tgt.append(line_array)\n",
    "#             print(line_array[0])\n",
    "            if len(line_array)>1:\n",
    "                src_words.append(line_array[0])\n",
    "                tgt_words.append(line_array[1])\n",
    "        f.close()\n",
    "    return src_words,tgt_words,all_src_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "src_words,tgt_words,all_src_tgt=get_sentences_from_all(\"C:/Users/Hanane/Downloads/data_Wiki_NER_filtered.sent-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src_words,tgt_words=get_sentences_from_src_tgt(\"C:/Users/Hanane/Downloads/data_Wiki_NER_filtered.sent-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\" \" ( Editions législatives , novembre 2017', 'O O O O O O B-DATE I-DATE'),\n",
       " ('\" \" ( Editions législatives , novembre', 'O O O O O O B-DATE'),\n",
       " ('\" \" , avec Robyn', 'O O O O B-PEOPLE'),\n",
       " ('\" \" ( Editions législatives , novembre 2017 )',\n",
       "  'O O O O O O B-DATE I-DATE O'),\n",
       " ('\" \" , avec Robyn , a', 'O O O O B-PEOPLE O O'),\n",
       " ('\" \" , jouée au Théâtre des Déchargeurs à',\n",
       "  'O O O O O B-MISC I-MISC I-MISC O'),\n",
       " ('\" \" , le 14 février 2011. Khemis El',\n",
       "  'O O O O B-DATE I-DATE O B-LOC_CITY I-LOC_CITY'),\n",
       " ('\" \" , avec Robyn , a été', 'O O O O B-PEOPLE O O O'),\n",
       " ('\" \" , avec Robyn ,', 'O O O O B-PEOPLE O'),\n",
       " ('\" \" , avec Robyn , a été publié', 'O O O O B-PEOPLE O O O O')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work with all_src_tgt instead of zipping the whole data set ==> and try to get words and tags by slicing the arrays instead of comprehensive lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in zip\n",
      "zip 319.613s.\n",
      "shuffle 71.278s.\n",
      "build train-valid-test 1.522s.\n",
      "build valid 509.012s.\n",
      "build test 585.912s.\n",
      "build train 3141.623s.\n"
     ]
    }
   ],
   "source": [
    "print(\"in zip\")\n",
    "t0=time()\n",
    "whole_dataset=list(zip(src_words,tgt_words))\n",
    "print(\"zip %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "    \n",
    "t0=time()\n",
    "random.shuffle(whole_dataset)\n",
    "print(\"shuffle %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "t0=time()\n",
    "train_size=int(0.8*len(whole_dataset))\n",
    "valid_size=int(0.9*len(whole_dataset))\n",
    "train_set=whole_dataset[:train_size]\n",
    "valid_set=whole_dataset[train_size:valid_size]\n",
    "test_set=whole_dataset[valid_size:]\n",
    "print(\"build train-valid-test %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "\n",
    "t0=time()\n",
    "valid_words=[elem[0] for elem in valid_set]\n",
    "valid_tags=[elem[1] for elem in valid_set]\n",
    "print(\"build valid %0.3fs.\" % (time() - t0),,\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "t0=time()\n",
    "test_words=[elem[0] for elem in test_set]\n",
    "test_tags=[elem[1] for elem in test_set]\n",
    "print(\"build test %0.2fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)\n",
    "\n",
    "t0=time()\n",
    "train_words=[elem[0] for elem in train_set]\n",
    "train_tags=[elem[1] for elem in train_set]\n",
    "print(\"build train %0.3fs.\" % (time() - t0),\" %0.1fs.\" % (time() - t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O O O O O O B-LOC_CITY',\n",
       " 'B-DATE I-DATE I-DATE O B-LOC_CITY',\n",
       " 'O O O O B-MISC I-MISC O',\n",
       " 'B-DATE O O O O O',\n",
       " 'I-PEOPLE O B-PEOPLE I-PEOPLE',\n",
       " 'O O B-LOC_COUNTRY O',\n",
       " 'O O O O O O B-DATE O',\n",
       " 'O O O O O O O O B-MISC',\n",
       " 'O O O O O O B-LOC_CITY O',\n",
       " 'O O B-PEOPLE I-PEOPLE O O O O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_to_txt(train_words,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_train_words_bitext_large_2.txt\")\n",
    "save_to_txt(train_tags,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_train_tags_bitext_large_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_to_txt(valid_words,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_valid_words_bitext_large_2.txt\")\n",
    "save_to_txt(valid_tags,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_valid_tags_bitext_large_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_to_txt(test_words,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_test_words_bitext_large_2.txt\")\n",
    "save_to_txt(test_tags,\"../dataset/train_valid/wiki_NER_semi_auto/wiki_test_tags_bitext_large_2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vocabularies ==> word, tag and tkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 14:40:27.258395: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-08-04 14:40:27.259954: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-08-04 14:40:40.039696: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2020-08-04 14:40:40.040643: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-04 14:40:40.055900: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-79FP3JFQ\n",
      "2020-08-04 14:40:40.057207: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-79FP3JFQ\n",
      "2020-08-04 14:40:40.065286: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#254294 rows\n",
    "path=\"../dataset/train_valid/wiki_NER_semi_auto/\"\n",
    "!onmt-build-vocab --size 1000000 --save_vocab ../dataset/train_valid/wiki_NER_semi_auto/wiki-src-train-vocab.txt ../dataset/train_valid/wiki_NER_semi_auto/wiki_train_words_bitext_large_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 15:02:09.899109: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-08-04 15:02:09.899935: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-08-04 15:02:17.243072: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2020-08-04 15:02:17.243944: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-04 15:02:17.257385: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-79FP3JFQ\n",
      "2020-08-04 15:02:17.258589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-79FP3JFQ\n",
      "2020-08-04 15:02:17.259719: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!onmt-build-vocab --size 1000 --save_vocab ../dataset/train_valid/wiki_NER_semi_auto/wiki-tgt-train-vocab.txt ../dataset/train_valid/wiki_NER_semi_auto/wiki_train_tags_bitext_large_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 15:11:37.211163: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-08-04 15:11:37.212027: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-08-04 15:11:44.211053: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2020-08-04 15:11:44.211852: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-04 15:11:44.224522: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-79FP3JFQ\n",
      "2020-08-04 15:11:44.225639: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-79FP3JFQ\n",
      "2020-08-04 15:11:44.226712: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!onmt-build-vocab --tokenizer_config ../config/char_tokenization.yml --size 1000000 --save_vocab ../dataset/train_valid/wiki_NER_semi_auto/wiki-src-train-tkt-vocab.txt ../dataset/train_valid/wiki_NER_semi_auto/wiki_train_words_bitext_large_2.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
