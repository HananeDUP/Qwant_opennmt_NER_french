{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opennmt_wiki_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5qAWvGT6L5",
        "colab_type": "text"
      },
      "source": [
        "# 1-Install OpenNMT from github-Qwant  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYaj7557TqHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "4c992ceb-e799-4a1d-c966-ff1e67a066e0"
      },
      "source": [
        "!git clone https://github.com/QwantResearch/OpenNMT-tf.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenNMT-tf'...\n",
            "remote: Enumerating objects: 23004, done.\u001b[K\n",
            "remote: Total 23004 (delta 0), reused 0 (delta 0), pack-reused 23004\u001b[K\n",
            "Receiving objects: 100% (23004/23004), 16.66 MiB | 22.96 MiB/s, done.\n",
            "Resolving deltas: 100% (18895/18895), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjKNdm7iT3hA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c573e99c-4f14-47f6-ce63-d4fb517f263e"
      },
      "source": [
        "!cd OpenNMT-tf; python -m pip install --force ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/OpenNMT-tf\n",
            "Collecting pyyaml<5.4,>=5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 4.6MB/s \n",
            "\u001b[?25hCollecting rouge<2,>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Collecting sacrebleu<2,>=1.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.2,>=2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/22/b81f319687fc59bf25e52e1f0959c88c93d55a4fd73eedd2e89e61499208/tensorflow-2.1.1-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 31kB/s \n",
            "\u001b[?25hCollecting tensorflow-addons<0.9,>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/f7/98c461ab7fcb4828f66a702f1af76811b9d3f47d62816f8cc57a9461b0da/tensorflow_addons-0.8.3-cp36-cp36m-manylinux2010_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 43.3MB/s \n",
            "\u001b[?25hCollecting ctranslate2<2,>=1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/69/5b0d865d127e987c1c0b9cf5c0d3b6a5e7b66525df114ecb3207f52ce21a/ctranslate2-1.13.0-cp36-cp36m-manylinux2010_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 238kB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/fc/aaa5096a948f2923d5e012409586274956368e00a6a4008412fb2807882d/pyonmttok-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 57.2MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting numpy<2.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 191kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 55.9MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/14/dc43f81adc543c435cfeb45dd4ac048a97a1eb621c2ccb68ab3d15118737/protobuf-3.12.4-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 50.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting astor>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 50.2MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/42/262913f967217874ae66734b52077833e2153b7b3a55a45bf996c7ee4833/grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 57.0MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 58.6MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26; python_version >= \"3\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
            "Collecting keras-preprocessing==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
            "Collecting scipy==1.4.1; python_version >= \"3\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1MB 116kB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hCollecting typeguard\n",
            "  Downloading https://files.pythonhosted.org/packages/52/33/3755584541a18d954389447bfd5f9cb7fa20dfbf5094829aee4a103e580c/typeguard-2.9.1-py3-none-any.whl\n",
            "Collecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/16/e9f5c5b86696da09298ea10c32d68ad8ea21f888e45b11aa9e615adda6c9/setuptools-49.2.1-py3-none-any.whl (789kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 51.5MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 53.3MB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
            "Collecting werkzeug>=0.11.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 52.6MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/02/00e06ffa98fd0f11f36f808511012fa1fce41e4f79fa35dc7c515364ed01/google_auth-1.20.0-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.5MB/s \n",
            "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.4MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 62.5MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.3MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
            "Collecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 61.9MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 58.8MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
            "Collecting pyasn1>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: OpenNMT-tf, pyyaml, gast, termcolor, absl-py, wrapt\n",
            "  Building wheel for OpenNMT-tf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for OpenNMT-tf: filename=OpenNMT_tf-2.8.0-cp36-none-any.whl size=135771 sha256=d1fd733618193de2908fdefaa74b7e019bc4f714a2daccb3007a0f87e0cb3914\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/80/31/508f29e22461db99910c4f61f4c17b20342fdb58df6e165788\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=b842a821ae1b0e74cc4ccefbd7ac1075ac85753b1d70868b5c9bcc4dc77efa32\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=1e9eebf3b3b8218070369825abbd1bbcb1703c0e2edd82d809a23cac63f315f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=96da3ca2002dd1a383d75dec2aea9821b7b54c28985d11efec95dd2066df4571\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.9.0-cp36-none-any.whl size=121931 sha256=97ff380aaf374465206f52d362b815d7db49ed22e74356ff9d76b09032e4e0a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=67498 sha256=38afd17dce559c8ec0791e08bd0ba0c002b0eb9afad27f8a5ae9f1da8bf1ad4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
            "Successfully built OpenNMT-tf pyyaml gast termcolor absl-py wrapt\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.17.2, but you'll have google-auth 1.20.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyyaml, six, rouge, portalocker, sacrebleu, numpy, tensorflow-estimator, google-pasta, setuptools, protobuf, gast, astor, termcolor, h5py, keras-applications, chardet, idna, certifi, urllib3, requests, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, grpcio, zipp, importlib-metadata, markdown, wheel, absl-py, tensorboard, keras-preprocessing, wrapt, scipy, opt-einsum, tensorflow, typeguard, tensorflow-addons, ctranslate2, pyonmttok, OpenNMT-tf\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Found existing installation: setuptools 49.2.0\n",
            "    Uninstalling setuptools-49.2.0:\n",
            "      Successfully uninstalled setuptools-49.2.0\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: certifi 2020.6.20\n",
            "    Uninstalling certifi-2020.6.20:\n",
            "      Successfully uninstalled certifi-2020.6.20\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: cachetools 4.1.1\n",
            "    Uninstalling cachetools-4.1.1:\n",
            "      Successfully uninstalled cachetools-4.1.1\n",
            "  Found existing installation: pyasn1 0.4.8\n",
            "    Uninstalling pyasn1-0.4.8:\n",
            "      Successfully uninstalled pyasn1-0.4.8\n",
            "  Found existing installation: rsa 4.6\n",
            "    Uninstalling rsa-4.6:\n",
            "      Successfully uninstalled rsa-4.6\n",
            "  Found existing installation: pyasn1-modules 0.2.8\n",
            "    Uninstalling pyasn1-modules-0.2.8:\n",
            "      Successfully uninstalled pyasn1-modules-0.2.8\n",
            "  Found existing installation: google-auth 1.17.2\n",
            "    Uninstalling google-auth-1.17.2:\n",
            "      Successfully uninstalled google-auth-1.17.2\n",
            "  Found existing installation: oauthlib 3.1.0\n",
            "    Uninstalling oauthlib-3.1.0:\n",
            "      Successfully uninstalled oauthlib-3.1.0\n",
            "  Found existing installation: requests-oauthlib 1.3.0\n",
            "    Uninstalling requests-oauthlib-1.3.0:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.0\n",
            "  Found existing installation: google-auth-oauthlib 0.4.1\n",
            "    Uninstalling google-auth-oauthlib-0.4.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.1\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: grpcio 1.30.0\n",
            "    Uninstalling grpcio-1.30.0:\n",
            "      Successfully uninstalled grpcio-1.30.0\n",
            "  Found existing installation: zipp 3.1.0\n",
            "    Uninstalling zipp-3.1.0:\n",
            "      Successfully uninstalled zipp-3.1.0\n",
            "  Found existing installation: importlib-metadata 1.7.0\n",
            "    Uninstalling importlib-metadata-1.7.0:\n",
            "      Successfully uninstalled importlib-metadata-1.7.0\n",
            "  Found existing installation: Markdown 3.2.2\n",
            "    Uninstalling Markdown-3.2.2:\n",
            "      Successfully uninstalled Markdown-3.2.2\n",
            "  Found existing installation: wheel 0.34.2\n",
            "    Uninstalling wheel-0.34.2:\n",
            "      Successfully uninstalled wheel-0.34.2\n",
            "  Found existing installation: absl-py 0.9.0\n",
            "    Uninstalling absl-py-0.9.0:\n",
            "      Successfully uninstalled absl-py-0.9.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Successfully installed OpenNMT-tf-2.8.0 absl-py-0.9.0 astor-0.8.1 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 ctranslate2-1.13.0 gast-0.2.2 google-auth-1.20.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 idna-2.10 importlib-metadata-1.7.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.2 numpy-1.19.1 oauthlib-3.1.0 opt-einsum-3.3.0 portalocker-2.0.0 protobuf-3.12.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyonmttok-1.18.5 pyyaml-5.3.1 requests-2.24.0 requests-oauthlib-1.3.0 rouge-1.0.0 rsa-4.6 sacrebleu-1.4.13 scipy-1.4.1 setuptools-49.2.1 six-1.15.0 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-addons-0.8.3 tensorflow-estimator-2.1.0 termcolor-1.1.0 typeguard-2.9.1 urllib3-1.25.10 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1 zipp-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0TL_JdTUCjN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a2b60162-01e0-4dfa-ba39-650f1c2f755b"
      },
      "source": [
        "!python -m pip install subword-nmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGPgOpwcUK6I",
        "colab_type": "text"
      },
      "source": [
        "# 2-Config and data files: clone my repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sueRvxBuTJN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "49219f5b-0d00-43ea-810d-03ebac078303"
      },
      "source": [
        "!git clone https://github.com/HananeDUP/Qwant_opennmt_NER_french.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Qwant_opennmt_NER_french'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Total 58 (delta 0), reused 0 (delta 0), pack-reused 58\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n",
            "Checking out files: 100% (13/13), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4wL2_alUBqF",
        "colab_type": "text"
      },
      "source": [
        "# 3-Get huge files (train) from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2IFJ-cZOILd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKwCZpcNOeQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6e289511-2abf-41da-8816-7fa21c592282"
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CccRrBzWQbB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/My\\ Drive/opennmt_qwant/wiki_train_tags_bitext_large_2.txt Qwant_opennmt_NER_french/dataset/train_valid\n",
        "!cp drive/My\\ Drive/opennmt_qwant/wiki_train_words_bitext_large_2.txt Qwant_opennmt_NER_french/dataset/train_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LxQa1JUgr3",
        "colab_type": "text"
      },
      "source": [
        "# 4-Launch model LstmCnnCrfTagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpewM6sIUzew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "955474e9-63b9-486f-8fec-8988c4cbc6dd"
      },
      "source": [
        "%%time\n",
        "!cd Qwant_opennmt_NER_french; onmt-main --model_type LstmCnnCrfTagger --config config/data_large_steps.yml --auto_config train --with_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 19:49:43.646679: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-08-04 19:49:43.646819: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2020-08-04 19:49:43.646837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "INFO:tensorflow:Creating model directory run/\n",
            "2020-08-04 19:49:44.891710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-04 19:49:44.945367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:44.945914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-04 19:49:44.946180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-04 19:49:45.187947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-04 19:49:45.317346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-04 19:49:45.364872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-04 19:49:45.624325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-04 19:49:45.670672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-04 19:49:46.182557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-04 19:49:46.182765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.183484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.184000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2020-08-04 19:49:46.184329: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-08-04 19:49:46.191023: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-08-04 19:49:46.191275: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16dabc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-04 19:49:46.191307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-04 19:49:46.334088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.335031: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16dad80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-04 19:49:46.335067: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-04 19:49:46.335907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.336657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-04 19:49:46.336731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-04 19:49:46.336754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-04 19:49:46.336772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-04 19:49:46.336790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-04 19:49:46.336811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-04 19:49:46.336828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-04 19:49:46.336848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-04 19:49:46.336919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.337639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.338116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2020-08-04 19:49:46.338387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-04 19:49:46.341009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-04 19:49:46.341040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
            "2020-08-04 19:49:46.341052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
            "2020-08-04 19:49:46.341399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.341941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-04 19:49:46.342476: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-04 19:49:46.342559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Using parameters:\n",
            "data:\n",
            "  eval_features_file: ./dataset/train_valid/wiki_valid_words_bitext_large_2.txt\n",
            "  eval_labels_file: ./dataset/train_valid/wiki_valid_tags_bitext_large_2.txt\n",
            "  source_1_vocabulary: ./dataset/train_valid/wiki-src-train-vocab.txt\n",
            "  source_2_vocabulary: ./dataset/train_valid/wiki-src-train-tkt-vocab.txt\n",
            "  target_vocabulary: ./dataset/train_valid/wiki-tgt-train-vocab.txt\n",
            "  train_features_file: ./dataset/train_valid/wiki_train_words_bitext_large_2.txt\n",
            "  train_labels_file: ./dataset/train_valid/wiki_train_tags_bitext_large_2.txt\n",
            "eval:\n",
            "  batch_size: 32\n",
            "  export_on_best: loss\n",
            "  external_evaluators: bleu\n",
            "infer:\n",
            "  batch_size: 16\n",
            "  length_bucket_width: null\n",
            "model_dir: run/\n",
            "params:\n",
            "  average_loss_in_time: false\n",
            "  learning_rate: 0.001\n",
            "  num_hypotheses: 1\n",
            "  optimizer: Adam\n",
            "score:\n",
            "  batch_size: 64\n",
            "train:\n",
            "  batch_size: 32\n",
            "  batch_type: examples\n",
            "  length_bucket_width: 1\n",
            "  max_step: 100000\n",
            "  sample_buffer_size: 500000\n",
            "  save_summary_steps: 200\n",
            "\n",
            "WARNING:tensorflow:No checkpoint to restore in run/\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Training on 16075931 examples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Number of model parameters: 27194530\n",
            "INFO:tensorflow:Number of model weights: 13 (trainable = 13, non trainable = 0)\n",
            "2020-08-04 19:51:02.296995: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 101718000 exceeds 10% of system memory.\n",
            "2020-08-04 19:51:02.363541: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 101718000 exceeds 10% of system memory.\n",
            "2020-08-04 19:51:02.388986: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 101718000 exceeds 10% of system memory.\n",
            "2020-08-04 19:51:02.410093: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 101718000 exceeds 10% of system memory.\n",
            "2020-08-04 19:51:02.434408: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 101718000 exceeds 10% of system memory.\n",
            "2020-08-04 19:51:05.950083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-04 19:51:26.793746: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 1 of 500000\n",
            "2020-08-04 19:51:27.456500: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 310131 of 500000\n",
            "2020-08-04 19:51:27.857809: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:199] Shuffle buffer filled.\n",
            "2020-08-04 19:51:31.101886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-1\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Step = 200 ; steps/s = 1.65, source words/s = 349, target words/s = 349 ; Learning rate = 0.001000 ; Loss = 4.467599\n",
            "INFO:tensorflow:Step = 400 ; steps/s = 23.56, source words/s = 5024, target words/s = 5024 ; Learning rate = 0.001000 ; Loss = 2.649165\n",
            "INFO:tensorflow:Step = 600 ; steps/s = 24.82, source words/s = 5221, target words/s = 5221 ; Learning rate = 0.001000 ; Loss = 3.859385\n",
            "INFO:tensorflow:Step = 800 ; steps/s = 25.00, source words/s = 5288, target words/s = 5288 ; Learning rate = 0.001000 ; Loss = 2.561897\n",
            "INFO:tensorflow:Step = 1000 ; steps/s = 23.29, source words/s = 4910, target words/s = 4910 ; Learning rate = 0.001000 ; Loss = 3.549614\n",
            "INFO:tensorflow:Step = 1200 ; steps/s = 24.97, source words/s = 5302, target words/s = 5302 ; Learning rate = 0.001000 ; Loss = 3.042501\n",
            "INFO:tensorflow:Step = 1400 ; steps/s = 24.42, source words/s = 5196, target words/s = 5196 ; Learning rate = 0.001000 ; Loss = 3.723469\n",
            "INFO:tensorflow:Step = 1600 ; steps/s = 23.72, source words/s = 5016, target words/s = 5016 ; Learning rate = 0.001000 ; Loss = 3.478810\n",
            "INFO:tensorflow:Step = 1800 ; steps/s = 24.96, source words/s = 5299, target words/s = 5299 ; Learning rate = 0.001000 ; Loss = 2.363075\n",
            "INFO:tensorflow:Step = 2000 ; steps/s = 25.22, source words/s = 5355, target words/s = 5355 ; Learning rate = 0.001000 ; Loss = 3.627398\n",
            "INFO:tensorflow:Step = 2200 ; steps/s = 25.21, source words/s = 5304, target words/s = 5304 ; Learning rate = 0.001000 ; Loss = 1.485132\n",
            "INFO:tensorflow:Step = 2400 ; steps/s = 24.85, source words/s = 5287, target words/s = 5287 ; Learning rate = 0.001000 ; Loss = 2.477244\n",
            "INFO:tensorflow:Step = 2600 ; steps/s = 25.07, source words/s = 5331, target words/s = 5331 ; Learning rate = 0.001000 ; Loss = 2.224063\n",
            "INFO:tensorflow:Step = 2800 ; steps/s = 25.82, source words/s = 5518, target words/s = 5518 ; Learning rate = 0.001000 ; Loss = 2.808862\n",
            "INFO:tensorflow:Step = 3000 ; steps/s = 25.93, source words/s = 5480, target words/s = 5480 ; Learning rate = 0.001000 ; Loss = 1.962201\n",
            "INFO:tensorflow:Step = 3200 ; steps/s = 24.88, source words/s = 5295, target words/s = 5295 ; Learning rate = 0.001000 ; Loss = 1.856204\n",
            "INFO:tensorflow:Step = 3400 ; steps/s = 23.74, source words/s = 5032, target words/s = 5032 ; Learning rate = 0.001000 ; Loss = 1.890197\n",
            "INFO:tensorflow:Step = 3600 ; steps/s = 24.80, source words/s = 5245, target words/s = 5245 ; Learning rate = 0.001000 ; Loss = 1.453129\n",
            "INFO:tensorflow:Step = 3800 ; steps/s = 24.99, source words/s = 5274, target words/s = 5274 ; Learning rate = 0.001000 ; Loss = 1.018089\n",
            "INFO:tensorflow:Step = 4000 ; steps/s = 24.11, source words/s = 5130, target words/s = 5130 ; Learning rate = 0.001000 ; Loss = 1.675555\n",
            "INFO:tensorflow:Step = 4200 ; steps/s = 24.52, source words/s = 5205, target words/s = 5205 ; Learning rate = 0.001000 ; Loss = 1.673855\n",
            "INFO:tensorflow:Step = 4400 ; steps/s = 25.11, source words/s = 5355, target words/s = 5355 ; Learning rate = 0.001000 ; Loss = 1.455139\n",
            "INFO:tensorflow:Step = 4600 ; steps/s = 25.58, source words/s = 5447, target words/s = 5447 ; Learning rate = 0.001000 ; Loss = 1.481367\n",
            "INFO:tensorflow:Step = 4800 ; steps/s = 25.93, source words/s = 5468, target words/s = 5468 ; Learning rate = 0.001000 ; Loss = 1.722747\n",
            "INFO:tensorflow:Step = 5000 ; steps/s = 25.40, source words/s = 5404, target words/s = 5404 ; Learning rate = 0.001000 ; Loss = 1.191805\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-5000\n",
            "INFO:tensorflow:Running evaluation for step 5000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.5000\n",
            "INFO:tensorflow:Evaluation result for step 5000: loss = 1.702142 ; perplexity = 5.485688 ; accuracy = 0.888539 ; bleu = 86.074054\n",
            "INFO:tensorflow:Exporting model to run/export/5000 (best loss so far: 1.702142)\n",
            "2020-08-04 20:21:09.462816: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "INFO:tensorflow:Assets written to: run/export/5000/assets\n",
            "INFO:tensorflow:Step = 5200 ; steps/s = 0.13, source words/s = 27, target words/s = 27 ; Learning rate = 0.001000 ; Loss = 2.506996\n",
            "INFO:tensorflow:Step = 5400 ; steps/s = 25.94, source words/s = 5491, target words/s = 5491 ; Learning rate = 0.001000 ; Loss = 2.166150\n",
            "INFO:tensorflow:Step = 5600 ; steps/s = 25.29, source words/s = 5350, target words/s = 5350 ; Learning rate = 0.001000 ; Loss = 1.800549\n",
            "INFO:tensorflow:Step = 5800 ; steps/s = 25.54, source words/s = 5434, target words/s = 5434 ; Learning rate = 0.001000 ; Loss = 1.142951\n",
            "INFO:tensorflow:Step = 6000 ; steps/s = 25.85, source words/s = 5472, target words/s = 5472 ; Learning rate = 0.001000 ; Loss = 1.656862\n",
            "INFO:tensorflow:Step = 6200 ; steps/s = 25.92, source words/s = 5483, target words/s = 5483 ; Learning rate = 0.001000 ; Loss = 1.240267\n",
            "INFO:tensorflow:Step = 6400 ; steps/s = 24.43, source words/s = 5162, target words/s = 5162 ; Learning rate = 0.001000 ; Loss = 1.271365\n",
            "INFO:tensorflow:Step = 6600 ; steps/s = 24.08, source words/s = 5097, target words/s = 5097 ; Learning rate = 0.001000 ; Loss = 1.401522\n",
            "INFO:tensorflow:Step = 6800 ; steps/s = 25.24, source words/s = 5363, target words/s = 5363 ; Learning rate = 0.001000 ; Loss = 1.573664\n",
            "INFO:tensorflow:Step = 7000 ; steps/s = 25.43, source words/s = 5422, target words/s = 5422 ; Learning rate = 0.001000 ; Loss = 1.948075\n",
            "INFO:tensorflow:Step = 7200 ; steps/s = 24.62, source words/s = 5195, target words/s = 5195 ; Learning rate = 0.001000 ; Loss = 1.893040\n",
            "INFO:tensorflow:Step = 7400 ; steps/s = 25.27, source words/s = 5369, target words/s = 5369 ; Learning rate = 0.001000 ; Loss = 1.482575\n",
            "INFO:tensorflow:Step = 7600 ; steps/s = 25.70, source words/s = 5431, target words/s = 5431 ; Learning rate = 0.001000 ; Loss = 1.034396\n",
            "INFO:tensorflow:Step = 7800 ; steps/s = 25.59, source words/s = 5453, target words/s = 5453 ; Learning rate = 0.001000 ; Loss = 0.992457\n",
            "INFO:tensorflow:Step = 8000 ; steps/s = 25.46, source words/s = 5365, target words/s = 5365 ; Learning rate = 0.001000 ; Loss = 1.341523\n",
            "INFO:tensorflow:Step = 8200 ; steps/s = 25.86, source words/s = 5473, target words/s = 5473 ; Learning rate = 0.001000 ; Loss = 1.809355\n",
            "INFO:tensorflow:Step = 8400 ; steps/s = 26.07, source words/s = 5485, target words/s = 5485 ; Learning rate = 0.001000 ; Loss = 1.171509\n",
            "INFO:tensorflow:Step = 8600 ; steps/s = 25.94, source words/s = 5548, target words/s = 5548 ; Learning rate = 0.001000 ; Loss = 2.257448\n",
            "INFO:tensorflow:Step = 8800 ; steps/s = 25.65, source words/s = 5479, target words/s = 5479 ; Learning rate = 0.001000 ; Loss = 2.140043\n",
            "INFO:tensorflow:Step = 9000 ; steps/s = 25.65, source words/s = 5422, target words/s = 5422 ; Learning rate = 0.001000 ; Loss = 0.926225\n",
            "INFO:tensorflow:Step = 9200 ; steps/s = 26.07, source words/s = 5555, target words/s = 5555 ; Learning rate = 0.001000 ; Loss = 2.690500\n",
            "INFO:tensorflow:Step = 9400 ; steps/s = 25.38, source words/s = 5409, target words/s = 5409 ; Learning rate = 0.001000 ; Loss = 1.665138\n",
            "INFO:tensorflow:Step = 9600 ; steps/s = 25.11, source words/s = 5259, target words/s = 5259 ; Learning rate = 0.001000 ; Loss = 1.196162\n",
            "INFO:tensorflow:Step = 9800 ; steps/s = 25.18, source words/s = 5354, target words/s = 5354 ; Learning rate = 0.001000 ; Loss = 1.554914\n",
            "INFO:tensorflow:Step = 10000 ; steps/s = 25.10, source words/s = 5360, target words/s = 5360 ; Learning rate = 0.001000 ; Loss = 1.220790\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-10000\n",
            "INFO:tensorflow:Running evaluation for step 10000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.10000\n",
            "INFO:tensorflow:Evaluation result for step 10000: loss = 1.472877 ; perplexity = 4.361764 ; accuracy = 0.897654 ; bleu = 87.403949\n",
            "INFO:tensorflow:Exporting model to run/export/10000 (best loss so far: 1.472877)\n",
            "INFO:tensorflow:Assets written to: run/export/10000/assets\n",
            "INFO:tensorflow:Step = 10200 ; steps/s = 0.13, source words/s = 26, target words/s = 26 ; Learning rate = 0.001000 ; Loss = 0.735124\n",
            "INFO:tensorflow:Step = 10400 ; steps/s = 24.57, source words/s = 5232, target words/s = 5232 ; Learning rate = 0.001000 ; Loss = 2.516218\n",
            "INFO:tensorflow:Step = 10600 ; steps/s = 24.87, source words/s = 5292, target words/s = 5292 ; Learning rate = 0.001000 ; Loss = 2.141390\n",
            "INFO:tensorflow:Step = 10800 ; steps/s = 23.82, source words/s = 5004, target words/s = 5004 ; Learning rate = 0.001000 ; Loss = 1.946158\n",
            "INFO:tensorflow:Step = 11000 ; steps/s = 25.68, source words/s = 5390, target words/s = 5390 ; Learning rate = 0.001000 ; Loss = 1.823977\n",
            "INFO:tensorflow:Step = 11200 ; steps/s = 26.11, source words/s = 5552, target words/s = 5552 ; Learning rate = 0.001000 ; Loss = 1.162694\n",
            "INFO:tensorflow:Step = 11400 ; steps/s = 24.82, source words/s = 5274, target words/s = 5274 ; Learning rate = 0.001000 ; Loss = 0.902539\n",
            "INFO:tensorflow:Step = 11600 ; steps/s = 26.39, source words/s = 5540, target words/s = 5540 ; Learning rate = 0.001000 ; Loss = 2.190773\n",
            "INFO:tensorflow:Step = 11800 ; steps/s = 25.17, source words/s = 5303, target words/s = 5303 ; Learning rate = 0.001000 ; Loss = 1.932650\n",
            "INFO:tensorflow:Step = 12000 ; steps/s = 25.72, source words/s = 5465, target words/s = 5465 ; Learning rate = 0.001000 ; Loss = 0.899893\n",
            "INFO:tensorflow:Step = 12200 ; steps/s = 25.02, source words/s = 5287, target words/s = 5287 ; Learning rate = 0.001000 ; Loss = 1.631501\n",
            "INFO:tensorflow:Step = 12400 ; steps/s = 26.19, source words/s = 5510, target words/s = 5510 ; Learning rate = 0.001000 ; Loss = 1.286821\n",
            "INFO:tensorflow:Step = 12600 ; steps/s = 25.43, source words/s = 5403, target words/s = 5403 ; Learning rate = 0.001000 ; Loss = 1.790488\n",
            "INFO:tensorflow:Step = 12800 ; steps/s = 26.13, source words/s = 5552, target words/s = 5552 ; Learning rate = 0.001000 ; Loss = 1.265458\n",
            "INFO:tensorflow:Step = 13000 ; steps/s = 25.32, source words/s = 5335, target words/s = 5335 ; Learning rate = 0.001000 ; Loss = 1.139972\n",
            "INFO:tensorflow:Step = 13200 ; steps/s = 26.36, source words/s = 5563, target words/s = 5563 ; Learning rate = 0.001000 ; Loss = 0.564218\n",
            "INFO:tensorflow:Step = 13400 ; steps/s = 25.65, source words/s = 5450, target words/s = 5450 ; Learning rate = 0.001000 ; Loss = 1.181578\n",
            "INFO:tensorflow:Step = 13600 ; steps/s = 25.57, source words/s = 5428, target words/s = 5428 ; Learning rate = 0.001000 ; Loss = 1.950735\n",
            "INFO:tensorflow:Step = 13800 ; steps/s = 25.92, source words/s = 5499, target words/s = 5499 ; Learning rate = 0.001000 ; Loss = 1.209373\n",
            "INFO:tensorflow:Step = 14000 ; steps/s = 25.32, source words/s = 5347, target words/s = 5347 ; Learning rate = 0.001000 ; Loss = 1.969227\n",
            "INFO:tensorflow:Step = 14200 ; steps/s = 24.56, source words/s = 5223, target words/s = 5223 ; Learning rate = 0.001000 ; Loss = 0.911181\n",
            "INFO:tensorflow:Step = 14400 ; steps/s = 26.09, source words/s = 5526, target words/s = 5526 ; Learning rate = 0.001000 ; Loss = 0.909322\n",
            "INFO:tensorflow:Step = 14600 ; steps/s = 25.27, source words/s = 5341, target words/s = 5341 ; Learning rate = 0.001000 ; Loss = 1.277916\n",
            "INFO:tensorflow:Step = 14800 ; steps/s = 25.53, source words/s = 5387, target words/s = 5387 ; Learning rate = 0.001000 ; Loss = 1.515596\n",
            "INFO:tensorflow:Step = 15000 ; steps/s = 25.54, source words/s = 5394, target words/s = 5394 ; Learning rate = 0.001000 ; Loss = 1.921941\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-15000\n",
            "INFO:tensorflow:Running evaluation for step 15000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.15000\n",
            "INFO:tensorflow:Evaluation result for step 15000: loss = 1.350213 ; perplexity = 3.858247 ; accuracy = 0.901317 ; bleu = 88.565817\n",
            "INFO:tensorflow:Exporting model to run/export/15000 (best loss so far: 1.350213)\n",
            "INFO:tensorflow:Assets written to: run/export/15000/assets\n",
            "INFO:tensorflow:Step = 15200 ; steps/s = 0.13, source words/s = 27, target words/s = 27 ; Learning rate = 0.001000 ; Loss = 0.810022\n",
            "INFO:tensorflow:Step = 15400 ; steps/s = 26.16, source words/s = 5574, target words/s = 5574 ; Learning rate = 0.001000 ; Loss = 1.956765\n",
            "INFO:tensorflow:Step = 15600 ; steps/s = 26.18, source words/s = 5503, target words/s = 5503 ; Learning rate = 0.001000 ; Loss = 0.825214\n",
            "INFO:tensorflow:Step = 15800 ; steps/s = 8.94, source words/s = 1898, target words/s = 1898 ; Learning rate = 0.001000 ; Loss = 0.822529\n",
            "INFO:tensorflow:Step = 16000 ; steps/s = 25.60, source words/s = 5423, target words/s = 5423 ; Learning rate = 0.001000 ; Loss = 1.549186\n",
            "INFO:tensorflow:Step = 16200 ; steps/s = 25.53, source words/s = 5448, target words/s = 5448 ; Learning rate = 0.001000 ; Loss = 0.950893\n",
            "INFO:tensorflow:Step = 16400 ; steps/s = 25.72, source words/s = 5435, target words/s = 5435 ; Learning rate = 0.001000 ; Loss = 0.718199\n",
            "INFO:tensorflow:Step = 16600 ; steps/s = 25.54, source words/s = 5409, target words/s = 5409 ; Learning rate = 0.001000 ; Loss = 2.418181\n",
            "INFO:tensorflow:Step = 16800 ; steps/s = 25.03, source words/s = 5313, target words/s = 5313 ; Learning rate = 0.001000 ; Loss = 0.663738\n",
            "INFO:tensorflow:Step = 17000 ; steps/s = 25.81, source words/s = 5529, target words/s = 5529 ; Learning rate = 0.001000 ; Loss = 2.025938\n",
            "INFO:tensorflow:Step = 17200 ; steps/s = 26.03, source words/s = 5514, target words/s = 5514 ; Learning rate = 0.001000 ; Loss = 2.160825\n",
            "INFO:tensorflow:Step = 17400 ; steps/s = 25.39, source words/s = 5317, target words/s = 5317 ; Learning rate = 0.001000 ; Loss = 0.799986\n",
            "INFO:tensorflow:Step = 17600 ; steps/s = 25.20, source words/s = 5367, target words/s = 5367 ; Learning rate = 0.001000 ; Loss = 1.641600\n",
            "INFO:tensorflow:Step = 17800 ; steps/s = 25.21, source words/s = 5299, target words/s = 5299 ; Learning rate = 0.001000 ; Loss = 0.881878\n",
            "INFO:tensorflow:Step = 18000 ; steps/s = 24.74, source words/s = 5285, target words/s = 5285 ; Learning rate = 0.001000 ; Loss = 0.898998\n",
            "INFO:tensorflow:Step = 18200 ; steps/s = 25.53, source words/s = 5379, target words/s = 5379 ; Learning rate = 0.001000 ; Loss = 1.456338\n",
            "INFO:tensorflow:Step = 18400 ; steps/s = 25.14, source words/s = 5308, target words/s = 5308 ; Learning rate = 0.001000 ; Loss = 0.696184\n",
            "INFO:tensorflow:Step = 18600 ; steps/s = 23.53, source words/s = 5015, target words/s = 5015 ; Learning rate = 0.001000 ; Loss = 1.838022\n",
            "INFO:tensorflow:Step = 18800 ; steps/s = 25.48, source words/s = 5410, target words/s = 5410 ; Learning rate = 0.001000 ; Loss = 1.140227\n",
            "INFO:tensorflow:Step = 19000 ; steps/s = 24.05, source words/s = 5079, target words/s = 5079 ; Learning rate = 0.001000 ; Loss = 1.063646\n",
            "INFO:tensorflow:Step = 19200 ; steps/s = 24.94, source words/s = 5254, target words/s = 5254 ; Learning rate = 0.001000 ; Loss = 1.478499\n",
            "INFO:tensorflow:Step = 19400 ; steps/s = 25.73, source words/s = 5433, target words/s = 5433 ; Learning rate = 0.001000 ; Loss = 1.562295\n",
            "INFO:tensorflow:Step = 19600 ; steps/s = 25.51, source words/s = 5411, target words/s = 5411 ; Learning rate = 0.001000 ; Loss = 1.153006\n",
            "INFO:tensorflow:Step = 19800 ; steps/s = 25.85, source words/s = 5483, target words/s = 5483 ; Learning rate = 0.001000 ; Loss = 1.600933\n",
            "INFO:tensorflow:Step = 20000 ; steps/s = 25.71, source words/s = 5422, target words/s = 5422 ; Learning rate = 0.001000 ; Loss = 0.864890\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-20000\n",
            "INFO:tensorflow:Running evaluation for step 20000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.20000\n",
            "INFO:tensorflow:Evaluation result for step 20000: loss = 1.302640 ; perplexity = 3.678996 ; accuracy = 0.903656 ; bleu = 88.904231\n",
            "INFO:tensorflow:Exporting model to run/export/20000 (best loss so far: 1.302640)\n",
            "INFO:tensorflow:Assets written to: run/export/20000/assets\n",
            "INFO:tensorflow:Step = 20200 ; steps/s = 0.13, source words/s = 27, target words/s = 27 ; Learning rate = 0.001000 ; Loss = 1.805798\n",
            "INFO:tensorflow:Step = 20400 ; steps/s = 24.74, source words/s = 5263, target words/s = 5263 ; Learning rate = 0.001000 ; Loss = 2.336684\n",
            "INFO:tensorflow:Step = 20600 ; steps/s = 25.43, source words/s = 5363, target words/s = 5363 ; Learning rate = 0.001000 ; Loss = 1.886819\n",
            "INFO:tensorflow:Step = 20800 ; steps/s = 25.05, source words/s = 5323, target words/s = 5323 ; Learning rate = 0.001000 ; Loss = 1.057579\n",
            "INFO:tensorflow:Step = 21000 ; steps/s = 24.94, source words/s = 5292, target words/s = 5292 ; Learning rate = 0.001000 ; Loss = 1.214339\n",
            "INFO:tensorflow:Step = 21200 ; steps/s = 25.12, source words/s = 5340, target words/s = 5340 ; Learning rate = 0.001000 ; Loss = 2.064349\n",
            "INFO:tensorflow:Step = 21400 ; steps/s = 25.69, source words/s = 5475, target words/s = 5475 ; Learning rate = 0.001000 ; Loss = 1.221363\n",
            "INFO:tensorflow:Step = 21600 ; steps/s = 25.76, source words/s = 5449, target words/s = 5449 ; Learning rate = 0.001000 ; Loss = 1.799910\n",
            "INFO:tensorflow:Step = 21800 ; steps/s = 24.97, source words/s = 5281, target words/s = 5281 ; Learning rate = 0.001000 ; Loss = 0.958440\n",
            "INFO:tensorflow:Step = 22000 ; steps/s = 24.83, source words/s = 5263, target words/s = 5263 ; Learning rate = 0.001000 ; Loss = 1.235817\n",
            "INFO:tensorflow:Step = 22200 ; steps/s = 25.76, source words/s = 5485, target words/s = 5485 ; Learning rate = 0.001000 ; Loss = 1.326973\n",
            "INFO:tensorflow:Step = 22400 ; steps/s = 26.04, source words/s = 5557, target words/s = 5557 ; Learning rate = 0.001000 ; Loss = 1.470130\n",
            "INFO:tensorflow:Step = 22600 ; steps/s = 24.78, source words/s = 5242, target words/s = 5242 ; Learning rate = 0.001000 ; Loss = 1.701128\n",
            "INFO:tensorflow:Step = 22800 ; steps/s = 25.73, source words/s = 5401, target words/s = 5401 ; Learning rate = 0.001000 ; Loss = 0.718192\n",
            "INFO:tensorflow:Step = 23000 ; steps/s = 24.59, source words/s = 5209, target words/s = 5209 ; Learning rate = 0.001000 ; Loss = 1.496339\n",
            "INFO:tensorflow:Step = 23200 ; steps/s = 24.90, source words/s = 5301, target words/s = 5301 ; Learning rate = 0.001000 ; Loss = 1.133097\n",
            "INFO:tensorflow:Step = 23400 ; steps/s = 25.81, source words/s = 5463, target words/s = 5463 ; Learning rate = 0.001000 ; Loss = 1.740309\n",
            "INFO:tensorflow:Step = 23600 ; steps/s = 25.61, source words/s = 5413, target words/s = 5413 ; Learning rate = 0.001000 ; Loss = 1.055225\n",
            "INFO:tensorflow:Step = 23800 ; steps/s = 25.52, source words/s = 5406, target words/s = 5406 ; Learning rate = 0.001000 ; Loss = 1.028143\n",
            "INFO:tensorflow:Step = 24000 ; steps/s = 24.87, source words/s = 5267, target words/s = 5267 ; Learning rate = 0.001000 ; Loss = 1.235313\n",
            "INFO:tensorflow:Step = 24200 ; steps/s = 25.84, source words/s = 5415, target words/s = 5415 ; Learning rate = 0.001000 ; Loss = 1.831330\n",
            "INFO:tensorflow:Step = 24400 ; steps/s = 25.76, source words/s = 5461, target words/s = 5461 ; Learning rate = 0.001000 ; Loss = 1.765849\n",
            "INFO:tensorflow:Step = 24600 ; steps/s = 25.84, source words/s = 5481, target words/s = 5481 ; Learning rate = 0.001000 ; Loss = 0.965958\n",
            "INFO:tensorflow:Step = 24800 ; steps/s = 25.98, source words/s = 5532, target words/s = 5532 ; Learning rate = 0.001000 ; Loss = 1.062982\n",
            "INFO:tensorflow:Step = 25000 ; steps/s = 25.48, source words/s = 5377, target words/s = 5377 ; Learning rate = 0.001000 ; Loss = 1.104362\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-25000\n",
            "INFO:tensorflow:Running evaluation for step 25000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.25000\n",
            "INFO:tensorflow:Evaluation result for step 25000: loss = 1.272198 ; perplexity = 3.568690 ; accuracy = 0.905517 ; bleu = 89.213724\n",
            "INFO:tensorflow:Exporting model to run/export/25000 (best loss so far: 1.272198)\n",
            "INFO:tensorflow:Assets written to: run/export/25000/assets\n",
            "INFO:tensorflow:Step = 25200 ; steps/s = 0.13, source words/s = 26, target words/s = 26 ; Learning rate = 0.001000 ; Loss = 1.932727\n",
            "INFO:tensorflow:Step = 25400 ; steps/s = 26.05, source words/s = 5552, target words/s = 5552 ; Learning rate = 0.001000 ; Loss = 1.736800\n",
            "INFO:tensorflow:Step = 25600 ; steps/s = 25.27, source words/s = 5340, target words/s = 5340 ; Learning rate = 0.001000 ; Loss = 2.497311\n",
            "INFO:tensorflow:Step = 25800 ; steps/s = 24.93, source words/s = 5269, target words/s = 5269 ; Learning rate = 0.001000 ; Loss = 2.821039\n",
            "INFO:tensorflow:Step = 26000 ; steps/s = 25.29, source words/s = 5385, target words/s = 5385 ; Learning rate = 0.001000 ; Loss = 0.845345\n",
            "INFO:tensorflow:Step = 26200 ; steps/s = 25.17, source words/s = 5332, target words/s = 5332 ; Learning rate = 0.001000 ; Loss = 0.790561\n",
            "INFO:tensorflow:Step = 26400 ; steps/s = 24.70, source words/s = 5224, target words/s = 5224 ; Learning rate = 0.001000 ; Loss = 1.508368\n",
            "INFO:tensorflow:Step = 26600 ; steps/s = 24.90, source words/s = 5267, target words/s = 5267 ; Learning rate = 0.001000 ; Loss = 1.573392\n",
            "INFO:tensorflow:Step = 26800 ; steps/s = 25.48, source words/s = 5401, target words/s = 5401 ; Learning rate = 0.001000 ; Loss = 0.682110\n",
            "INFO:tensorflow:Step = 27000 ; steps/s = 25.52, source words/s = 5406, target words/s = 5406 ; Learning rate = 0.001000 ; Loss = 0.629459\n",
            "INFO:tensorflow:Step = 27200 ; steps/s = 25.28, source words/s = 5388, target words/s = 5388 ; Learning rate = 0.001000 ; Loss = 1.737837\n",
            "INFO:tensorflow:Step = 27400 ; steps/s = 26.14, source words/s = 5528, target words/s = 5528 ; Learning rate = 0.001000 ; Loss = 1.334648\n",
            "INFO:tensorflow:Step = 27600 ; steps/s = 25.88, source words/s = 5462, target words/s = 5462 ; Learning rate = 0.001000 ; Loss = 1.429447\n",
            "INFO:tensorflow:Step = 27800 ; steps/s = 25.57, source words/s = 5412, target words/s = 5412 ; Learning rate = 0.001000 ; Loss = 1.580734\n",
            "INFO:tensorflow:Step = 28000 ; steps/s = 25.64, source words/s = 5439, target words/s = 5439 ; Learning rate = 0.001000 ; Loss = 1.083240\n",
            "INFO:tensorflow:Step = 28200 ; steps/s = 25.44, source words/s = 5393, target words/s = 5393 ; Learning rate = 0.001000 ; Loss = 1.536119\n",
            "INFO:tensorflow:Step = 28400 ; steps/s = 25.83, source words/s = 5451, target words/s = 5451 ; Learning rate = 0.001000 ; Loss = 1.085729\n",
            "INFO:tensorflow:Step = 28600 ; steps/s = 25.80, source words/s = 5453, target words/s = 5453 ; Learning rate = 0.001000 ; Loss = 1.790991\n",
            "INFO:tensorflow:Step = 28800 ; steps/s = 25.76, source words/s = 5429, target words/s = 5429 ; Learning rate = 0.001000 ; Loss = 1.194813\n",
            "INFO:tensorflow:Step = 29000 ; steps/s = 25.33, source words/s = 5373, target words/s = 5373 ; Learning rate = 0.001000 ; Loss = 0.776471\n",
            "INFO:tensorflow:Step = 29200 ; steps/s = 25.04, source words/s = 5304, target words/s = 5304 ; Learning rate = 0.001000 ; Loss = 1.140155\n",
            "INFO:tensorflow:Step = 29400 ; steps/s = 25.12, source words/s = 5354, target words/s = 5354 ; Learning rate = 0.001000 ; Loss = 1.035517\n",
            "INFO:tensorflow:Step = 29600 ; steps/s = 26.15, source words/s = 5506, target words/s = 5506 ; Learning rate = 0.001000 ; Loss = 1.943369\n",
            "INFO:tensorflow:Step = 29800 ; steps/s = 26.48, source words/s = 5555, target words/s = 5555 ; Learning rate = 0.001000 ; Loss = 1.880584\n",
            "INFO:tensorflow:Step = 30000 ; steps/s = 25.34, source words/s = 5401, target words/s = 5401 ; Learning rate = 0.001000 ; Loss = 1.493312\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-30000\n",
            "INFO:tensorflow:Running evaluation for step 30000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.30000\n",
            "INFO:tensorflow:Evaluation result for step 30000: loss = 1.249497 ; perplexity = 3.488589 ; accuracy = 0.906886 ; bleu = 89.689845\n",
            "INFO:tensorflow:Exporting model to run/export/30000 (best loss so far: 1.249497)\n",
            "INFO:tensorflow:Assets written to: run/export/30000/assets\n",
            "INFO:tensorflow:Step = 30200 ; steps/s = 0.13, source words/s = 27, target words/s = 27 ; Learning rate = 0.001000 ; Loss = 1.096901\n",
            "INFO:tensorflow:Step = 30400 ; steps/s = 25.49, source words/s = 5384, target words/s = 5384 ; Learning rate = 0.001000 ; Loss = 0.969065\n",
            "INFO:tensorflow:Step = 30600 ; steps/s = 25.39, source words/s = 5391, target words/s = 5391 ; Learning rate = 0.001000 ; Loss = 1.688548\n",
            "INFO:tensorflow:Step = 30800 ; steps/s = 25.93, source words/s = 5472, target words/s = 5472 ; Learning rate = 0.001000 ; Loss = 0.817809\n",
            "INFO:tensorflow:Step = 31000 ; steps/s = 25.92, source words/s = 5519, target words/s = 5519 ; Learning rate = 0.001000 ; Loss = 1.793914\n",
            "INFO:tensorflow:Step = 31200 ; steps/s = 24.85, source words/s = 5265, target words/s = 5265 ; Learning rate = 0.001000 ; Loss = 1.951136\n",
            "INFO:tensorflow:Step = 31400 ; steps/s = 9.24, source words/s = 1958, target words/s = 1958 ; Learning rate = 0.001000 ; Loss = 1.467063\n",
            "INFO:tensorflow:Step = 31600 ; steps/s = 26.03, source words/s = 5518, target words/s = 5518 ; Learning rate = 0.001000 ; Loss = 1.154907\n",
            "INFO:tensorflow:Step = 31800 ; steps/s = 25.57, source words/s = 5420, target words/s = 5420 ; Learning rate = 0.001000 ; Loss = 1.692799\n",
            "INFO:tensorflow:Step = 32000 ; steps/s = 25.86, source words/s = 5416, target words/s = 5416 ; Learning rate = 0.001000 ; Loss = 1.395068\n",
            "INFO:tensorflow:Step = 32200 ; steps/s = 24.75, source words/s = 5271, target words/s = 5271 ; Learning rate = 0.001000 ; Loss = 1.176012\n",
            "INFO:tensorflow:Step = 32400 ; steps/s = 25.03, source words/s = 5278, target words/s = 5278 ; Learning rate = 0.001000 ; Loss = 0.936083\n",
            "INFO:tensorflow:Step = 32600 ; steps/s = 25.98, source words/s = 5548, target words/s = 5548 ; Learning rate = 0.001000 ; Loss = 1.315383\n",
            "INFO:tensorflow:Step = 32800 ; steps/s = 25.52, source words/s = 5381, target words/s = 5381 ; Learning rate = 0.001000 ; Loss = 0.811761\n",
            "INFO:tensorflow:Step = 33000 ; steps/s = 24.32, source words/s = 5124, target words/s = 5124 ; Learning rate = 0.001000 ; Loss = 1.345921\n",
            "INFO:tensorflow:Step = 33200 ; steps/s = 24.96, source words/s = 5298, target words/s = 5298 ; Learning rate = 0.001000 ; Loss = 0.879790\n",
            "INFO:tensorflow:Step = 33400 ; steps/s = 25.29, source words/s = 5414, target words/s = 5414 ; Learning rate = 0.001000 ; Loss = 1.361296\n",
            "INFO:tensorflow:Step = 33600 ; steps/s = 25.89, source words/s = 5497, target words/s = 5497 ; Learning rate = 0.001000 ; Loss = 1.003678\n",
            "INFO:tensorflow:Step = 33800 ; steps/s = 25.76, source words/s = 5432, target words/s = 5432 ; Learning rate = 0.001000 ; Loss = 1.178171\n",
            "INFO:tensorflow:Step = 34000 ; steps/s = 25.56, source words/s = 5443, target words/s = 5443 ; Learning rate = 0.001000 ; Loss = 1.398541\n",
            "INFO:tensorflow:Step = 34200 ; steps/s = 25.83, source words/s = 5493, target words/s = 5493 ; Learning rate = 0.001000 ; Loss = 1.584162\n",
            "INFO:tensorflow:Step = 34400 ; steps/s = 26.39, source words/s = 5595, target words/s = 5595 ; Learning rate = 0.001000 ; Loss = 0.907180\n",
            "INFO:tensorflow:Step = 34600 ; steps/s = 26.45, source words/s = 5607, target words/s = 5607 ; Learning rate = 0.001000 ; Loss = 0.863351\n",
            "INFO:tensorflow:Step = 34800 ; steps/s = 26.09, source words/s = 5527, target words/s = 5527 ; Learning rate = 0.001000 ; Loss = 1.484568\n",
            "INFO:tensorflow:Step = 35000 ; steps/s = 25.63, source words/s = 5475, target words/s = 5475 ; Learning rate = 0.001000 ; Loss = 1.865039\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-35000\n",
            "INFO:tensorflow:Running evaluation for step 35000\n",
            "INFO:tensorflow:Evaluation predictions saved to run/eval/predictions.txt.35000\n",
            "INFO:tensorflow:Evaluation result for step 35000: loss = 1.227973 ; perplexity = 3.414303 ; accuracy = 0.907674 ; bleu = 89.753528\n",
            "INFO:tensorflow:Exporting model to run/export/35000 (best loss so far: 1.227973)\n",
            "INFO:tensorflow:Assets written to: run/export/35000/assets\n",
            "INFO:tensorflow:Step = 35200 ; steps/s = 0.13, source words/s = 27, target words/s = 27 ; Learning rate = 0.001000 ; Loss = 1.860567\n",
            "INFO:tensorflow:Step = 35400 ; steps/s = 25.78, source words/s = 5469, target words/s = 5469 ; Learning rate = 0.001000 ; Loss = 1.595722\n",
            "INFO:tensorflow:Step = 35600 ; steps/s = 26.10, source words/s = 5523, target words/s = 5523 ; Learning rate = 0.001000 ; Loss = 2.593393\n",
            "INFO:tensorflow:Step = 35800 ; steps/s = 23.90, source words/s = 5120, target words/s = 5120 ; Learning rate = 0.001000 ; Loss = 0.797795\n",
            "INFO:tensorflow:Step = 36000 ; steps/s = 25.03, source words/s = 5273, target words/s = 5273 ; Learning rate = 0.001000 ; Loss = 0.880080\n",
            "INFO:tensorflow:Step = 36200 ; steps/s = 25.18, source words/s = 5337, target words/s = 5337 ; Learning rate = 0.001000 ; Loss = 1.838088\n",
            "INFO:tensorflow:Step = 36400 ; steps/s = 25.96, source words/s = 5528, target words/s = 5528 ; Learning rate = 0.001000 ; Loss = 1.383438\n",
            "INFO:tensorflow:Step = 36600 ; steps/s = 24.95, source words/s = 5268, target words/s = 5268 ; Learning rate = 0.001000 ; Loss = 0.605130\n",
            "INFO:tensorflow:Step = 36800 ; steps/s = 24.65, source words/s = 5190, target words/s = 5190 ; Learning rate = 0.001000 ; Loss = 2.458687\n",
            "INFO:tensorflow:Step = 37000 ; steps/s = 25.14, source words/s = 5340, target words/s = 5340 ; Learning rate = 0.001000 ; Loss = 0.961189\n",
            "INFO:tensorflow:Step = 37200 ; steps/s = 24.22, source words/s = 5127, target words/s = 5127 ; Learning rate = 0.001000 ; Loss = 1.246982\n",
            "INFO:tensorflow:Step = 37400 ; steps/s = 25.28, source words/s = 5327, target words/s = 5327 ; Learning rate = 0.001000 ; Loss = 0.642955\n",
            "INFO:tensorflow:Step = 37600 ; steps/s = 25.23, source words/s = 5376, target words/s = 5376 ; Learning rate = 0.001000 ; Loss = 0.588859\n",
            "INFO:tensorflow:Step = 37800 ; steps/s = 25.95, source words/s = 5542, target words/s = 5542 ; Learning rate = 0.001000 ; Loss = 2.826195\n",
            "INFO:tensorflow:Step = 38000 ; steps/s = 25.59, source words/s = 5404, target words/s = 5404 ; Learning rate = 0.001000 ; Loss = 1.238236\n",
            "INFO:tensorflow:Step = 38200 ; steps/s = 25.19, source words/s = 5300, target words/s = 5300 ; Learning rate = 0.001000 ; Loss = 1.052456\n",
            "INFO:tensorflow:Step = 38400 ; steps/s = 25.48, source words/s = 5430, target words/s = 5430 ; Learning rate = 0.001000 ; Loss = 0.999079\n",
            "INFO:tensorflow:Step = 38600 ; steps/s = 24.67, source words/s = 5210, target words/s = 5210 ; Learning rate = 0.001000 ; Loss = 2.313054\n",
            "INFO:tensorflow:Step = 38800 ; steps/s = 25.01, source words/s = 5321, target words/s = 5321 ; Learning rate = 0.001000 ; Loss = 1.127387\n",
            "INFO:tensorflow:Step = 39000 ; steps/s = 25.09, source words/s = 5322, target words/s = 5322 ; Learning rate = 0.001000 ; Loss = 2.262524\n",
            "INFO:tensorflow:Step = 39200 ; steps/s = 25.72, source words/s = 5403, target words/s = 5403 ; Learning rate = 0.001000 ; Loss = 0.741839\n",
            "INFO:tensorflow:Step = 39400 ; steps/s = 25.18, source words/s = 5338, target words/s = 5338 ; Learning rate = 0.001000 ; Loss = 1.227372\n",
            "INFO:tensorflow:Step = 39600 ; steps/s = 25.35, source words/s = 5382, target words/s = 5382 ; Learning rate = 0.001000 ; Loss = 1.869748\n",
            "INFO:tensorflow:Step = 39800 ; steps/s = 25.55, source words/s = 5403, target words/s = 5403 ; Learning rate = 0.001000 ; Loss = 1.759528\n",
            "INFO:tensorflow:Step = 40000 ; steps/s = 25.65, source words/s = 5458, target words/s = 5458 ; Learning rate = 0.001000 ; Loss = 0.514292\n",
            "INFO:tensorflow:Saved checkpoint run/ckpt-40000\n",
            "INFO:tensorflow:Running evaluation for step 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-65qo7HWsyG",
        "colab_type": "text"
      },
      "source": [
        "# 5-Infer model LstmCnnCrfTagger: test files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dehkFlEWuzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "9bea195b-d5be-4bc6-ab5f-230e8f23b8e0"
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; mkdir dataset/test; onmt-main --config config/data_large_steps.yml --auto_config infer --features_file dataset/train_valid/wiki_test_words_bitext_large_2.txt > dataset/test/test_tag_res.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset/test’: File exists\n",
            "2020-08-04 17:49:33.532148: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2020-08-04 17:49:33.532288: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2020-08-04 17:49:33.532314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "INFO:tensorflow:Loading model description from run/model_description.py\n",
            "INFO:tensorflow:Using parameters:\n",
            "data:\n",
            "  eval_features_file: ./dataset/train_valid/wiki_valid_words_bitext_large_2.txt\n",
            "  eval_labels_file: ./dataset/train_valid/wiki_valid_tags_bitext_large_2.txt\n",
            "  source_1_vocabulary: ./dataset/train_valid/wiki-src-train-vocab.txt\n",
            "  source_2_vocabulary: ./dataset/train_valid/wiki-src-train-tkt-vocab.txt\n",
            "  target_vocabulary: ./dataset/train_valid/wiki-tgt-train-vocab.txt\n",
            "  train_features_file: ./dataset/train_valid/wiki_train_words_bitext_large_2.txt\n",
            "  train_labels_file: ./dataset/train_valid/wiki_train_tags_bitext_large_2.txt\n",
            "eval:\n",
            "  batch_size: 32\n",
            "  export_on_best: loss\n",
            "  external_evaluators: bleu\n",
            "infer:\n",
            "  batch_size: 16\n",
            "  length_bucket_width: null\n",
            "model_dir: run/\n",
            "params:\n",
            "  average_loss_in_time: false\n",
            "  learning_rate: 0.001\n",
            "  num_hypotheses: 1\n",
            "  optimizer: Adam\n",
            "score:\n",
            "  batch_size: 64\n",
            "train:\n",
            "  batch_size: 32\n",
            "  batch_type: examples\n",
            "  length_bucket_width: 1\n",
            "  max_step: 200\n",
            "  sample_buffer_size: 500000\n",
            "  save_summary_steps: 100\n",
            "\n",
            "2020-08-04 17:49:35.645422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-04 17:49:35.648242: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-08-04 17:49:35.648307: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (444d86035267): /proc/driver/nvidia/version does not exist\n",
            "2020-08-04 17:49:35.648631: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-08-04 17:49:35.653311: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200135000 Hz\n",
            "2020-08-04 17:49:35.653541: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19c8d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-04 17:49:35.653578: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "INFO:tensorflow:Restored checkpoint run/ckpt-200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07B8SVLtXtbO",
        "colab_type": "text"
      },
      "source": [
        "# 6 - Manual export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GfDimjmXoFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cd Qwant_opennmt_NER_french; onmt-main --config config/data_small_steps.yml --auto_config export --export_dir ./export_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPipQD8co1Lt",
        "colab_type": "text"
      },
      "source": [
        "# 7- Launch model manually (.py):  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw-NvpNGo3m7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cd Qwant_openNMT; onmt-main --model seq_tagger_updated.py --config config/data_small_steps.yml train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1uHuKGor6h",
        "colab_type": "text"
      },
      "source": [
        "# 8-GITHUB:  \n",
        "\n",
        "1.   push saved model (in export folder)\n",
        "2.   and all files generated in run folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l8Kv-n8m7Aq",
        "colab_type": "text"
      },
      "source": [
        "Github push ==> remove large files before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_d5LxXOnA3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm Qwant_opennmt_NER_french/dataset/train_valid/wiki_train_tags_bitext_large_2.txt\n",
        "!rm Qwant_opennmt_NER_french/dataset/train_valid/wiki_train_words_bitext_large_2.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZY1RebfpMQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; git config --global user.email \"hanane.moualil@gmail.com\"\n",
        "!cd Qwant_opennmt_NER_french; git config --global user.name \"HananeDUP\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpXFnP6DtP1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; git add ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZD_ILRHtSdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "134a5814-e77b-480e-f317-b3a4e5ebf006"
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; git commit -m \"from googlecolab export auto (in run/export/) and manual (in my-models/tp_export)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master 09bda34] from googlecolab export auto (in run/export/) and manual (in my-models/tp_export)\n",
            " 20 files changed, 4530719 insertions(+)\n",
            " create mode 100644 dataset/test/test_tag_res.txt\n",
            " create mode 100644 run/__pycache__/model_description.cpython-36.pyc\n",
            " create mode 100644 run/checkpoint\n",
            " create mode 100644 run/ckpt-1.data-00000-of-00001\n",
            " create mode 100644 run/ckpt-1.index\n",
            " create mode 100644 run/ckpt-200.data-00000-of-00001\n",
            " create mode 100644 run/ckpt-200.index\n",
            " create mode 100644 run/eval/events.out.tfevents.1596558928.444d86035267.1375.24.v2\n",
            " create mode 100644 run/eval/predictions.txt.200\n",
            " create mode 100644 run/events.out.tfevents.1596558930.444d86035267.1375.297.v2\n",
            " create mode 100644 run/export/200/assets/wiki-src-train-tkt-vocab.txt\n",
            " create mode 100644 run/export/200/assets/wiki-src-train-vocab.txt\n",
            " create mode 100644 run/export/200/assets/wiki-tgt-train-vocab.txt\n",
            " create mode 100644 run/export/200/saved_model.pb\n",
            " create mode 100644 run/export/200/variables/variables.data-00000-of-00001\n",
            " create mode 100644 run/export/200/variables/variables.index\n",
            " create mode 100644 run/model_description.py\n",
            " create mode 100644 run/model_examples_inputter_features_inputter_inputters_0_embedding.txt\n",
            " create mode 100644 run/model_examples_inputter_features_inputter_inputters_1_char_embedding.txt\n",
            " create mode 100644 run/projector_config.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DMulPMWtWnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cd Qwant_openNMT; git remote remove originMDP\n",
        "!cd Qwant_opennmt_NER_french; git remote add origin https://HananeDUP:mdp@github.com/HananeDUP/Qwant_opennmt_NER_french.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrdfwHGFy8Ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "343f38a0-ad15-4a54-a73f-76502dfa4b00"
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "nothing to commit, working tree clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcH85gPAtdRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; git remote rm origin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8b9onLtf8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "6434fc22-ba5f-45af-9ebf-759ce5c016d8"
      },
      "source": [
        "!cd Qwant_opennmt_NER_french; git push -u origin master\n",
        "# it doesn't work because of large files in run directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 28, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (26/26), done.\n",
            "Writing objects: 100% (28/28), 142.07 MiB | 6.16 MiB/s, done.\n",
            "Total 28 (delta 4), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (4/4), completed with 1 local object.\u001b[K\n",
            "remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "remote: error: Trace: 31d8b25f845b26a8b17d7b237157dc4e\u001b[K\n",
            "remote: error: See http://git.io/iEPt8g for more information.\u001b[K\n",
            "remote: error: File run/ckpt-1.data-00000-of-00001 is 311.22 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
            "remote: error: File run/ckpt-200.data-00000-of-00001 is 311.22 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
            "remote: error: File run/export/200/variables/variables.data-00000-of-00001 is 103.75 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
            "To https://github.com/HananeDUP/Qwant_opennmt_NER_french.git\n",
            " ! [remote rejected] master -> master (pre-receive hook declined)\n",
            "error: failed to push some refs to 'https://HananeDUP:NicoNora1617@github.com/HananeDUP/Qwant_opennmt_NER_french.git'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip9t4hVo6nRy",
        "colab_type": "text"
      },
      "source": [
        "# 9-Send run folder and test results to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyue-fO13IJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "1d89f63b-8a0d-469b-c20d-571978b4212f"
      },
      "source": [
        "# !tar jcvf Qwant_opennmt_NER_french/run_tar_bz2 Qwant_opennmt_NER_french/run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Qwant_opennmt_NER_french/run/\n",
            "Qwant_opennmt_NER_french/run/__pycache__/\n",
            "Qwant_opennmt_NER_french/run/__pycache__/model_description.cpython-36.pyc\n",
            "Qwant_opennmt_NER_french/run/export/\n",
            "Qwant_opennmt_NER_french/run/export/200/\n",
            "Qwant_opennmt_NER_french/run/export/200/variables/\n",
            "Qwant_opennmt_NER_french/run/export/200/variables/variables.index\n",
            "Qwant_opennmt_NER_french/run/export/200/variables/variables.data-00000-of-00001\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/wiki-tgt-train-vocab.txt\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/wiki-src-train-vocab.txt\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/wiki-src-train-tkt-vocab.txt\n",
            "Qwant_opennmt_NER_french/run/export/200/saved_model.pb\n",
            "Qwant_opennmt_NER_french/run/ckpt-1.index\n",
            "Qwant_opennmt_NER_french/run/ckpt-200.index\n",
            "Qwant_opennmt_NER_french/run/checkpoint\n",
            "Qwant_opennmt_NER_french/run/model_examples_inputter_features_inputter_inputters_1_char_embedding.txt\n",
            "Qwant_opennmt_NER_french/run/eval/\n",
            "Qwant_opennmt_NER_french/run/eval/events.out.tfevents.1596558928.444d86035267.1375.24.v2\n",
            "Qwant_opennmt_NER_french/run/eval/predictions.txt.200\n",
            "Qwant_opennmt_NER_french/run/events.out.tfevents.1596558930.444d86035267.1375.297.v2\n",
            "Qwant_opennmt_NER_french/run/ckpt-200.data-00000-of-00001\n",
            "Qwant_opennmt_NER_french/run/ckpt-1.data-00000-of-00001\n",
            "Qwant_opennmt_NER_french/run/model_description.py\n",
            "Qwant_opennmt_NER_french/run/model_examples_inputter_features_inputter_inputters_0_embedding.txt\n",
            "Qwant_opennmt_NER_french/run/projector_config.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKbWBcnz9CB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "4c485818-43a2-4a77-a04d-105dde94a9b6"
      },
      "source": [
        "!tar jcvf drive/My\\ Drive/opennmt_qwant/run_google_colab_large.tar.bz2 Qwant_opennmt_NER_french/run\n",
        "# !cp -r Qwant_opennmt_NER_french/run drive/My\\ Drive/opennmt_qwant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Qwant_opennmt_NER_french/run/\n",
            "Qwant_opennmt_NER_french/run/__pycache__/\n",
            "Qwant_opennmt_NER_french/run/__pycache__/model_description.cpython-36.pyc\n",
            "Qwant_opennmt_NER_french/run/export/\n",
            "Qwant_opennmt_NER_french/run/export/200/\n",
            "Qwant_opennmt_NER_french/run/export/200/variables/\n",
            "Qwant_opennmt_NER_french/run/export/200/variables/variables.index\n",
            "Qwant_opennmt_NER_french/run/export/200/variables/variables.data-00000-of-00001\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/wiki-tgt-train-vocab.txt\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/wiki-src-train-vocab.txt\n",
            "Qwant_opennmt_NER_french/run/export/200/assets/wiki-src-train-tkt-vocab.txt\n",
            "Qwant_opennmt_NER_french/run/export/200/saved_model.pb\n",
            "Qwant_opennmt_NER_french/run/ckpt-1.index\n",
            "Qwant_opennmt_NER_french/run/ckpt-200.index\n",
            "Qwant_opennmt_NER_french/run/checkpoint\n",
            "Qwant_opennmt_NER_french/run/model_examples_inputter_features_inputter_inputters_1_char_embedding.txt\n",
            "Qwant_opennmt_NER_french/run/eval/\n",
            "Qwant_opennmt_NER_french/run/eval/events.out.tfevents.1596558928.444d86035267.1375.24.v2\n",
            "Qwant_opennmt_NER_french/run/eval/predictions.txt.200\n",
            "Qwant_opennmt_NER_french/run/events.out.tfevents.1596558930.444d86035267.1375.297.v2\n",
            "Qwant_opennmt_NER_french/run/ckpt-200.data-00000-of-00001\n",
            "Qwant_opennmt_NER_french/run/ckpt-1.data-00000-of-00001\n",
            "Qwant_opennmt_NER_french/run/model_description.py\n",
            "Qwant_opennmt_NER_french/run/model_examples_inputter_features_inputter_inputters_0_embedding.txt\n",
            "Qwant_opennmt_NER_french/run/projector_config.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2SwucBwppx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar jcvf drive/My\\ Drive/opennmt_qwant/run_export_last.tar.bz2 Qwant_opennmt_NER_french/run/export/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXmBKY6vxjnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar jcvf drive/My\\ Drive/opennmt_qwant/run_eval.tar.bz2 Qwant_opennmt_NER_french/run/eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G64xf_ox1gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar jcvf drive/My\\ Drive/opennmt_qwant/run_cpkt.tar.bz2 Qwant_opennmt_NER_french/run/cpkt-*.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JojNnvZa4UgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm Qwant_opennmt_NER_french/run_tar_bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Um8flts00NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r Qwant_opennmt_NER_french/dataset/test drive/My\\ Drive/opennmt_qwant"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}