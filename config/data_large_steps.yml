model_dir: run/

data:
  train_features_file: ./dataset/train_valid/wiki_train_words_bitext.txt
  train_labels_file: ./dataset/train_valid/wiki_train_tags_bitext.txt
  eval_features_file: ./dataset/train_valid/wiki_valid_words_bitext.txt
  eval_labels_file: ./dataset/train_valid/wiki_valid_tags_bitext.txt
  source_1_vocabulary: ./dataset/train_valid/wiki-src-train-vocab.txt
  source_2_vocabulary: ./dataset/train_valid/wiki-src-train-tkt-vocab.txt
  target_vocabulary: ./dataset/train_valid/wiki-tgt-train-vocab.txt

score:
  batch_size: 10

infer:
 batch_size: 10

eval:
  batch_size: 10
  external_evaluators: bleu
  export_on_best: loss
  steps: 20000

params:
  learning_rate: 0.015
  optimizer: SGD
  dropout: 0.5
  optimizer_params:
    momentum: 0.9
  # (optional) Maximum gradients norm (default: None).
  clip_gradients: 5.0
  # (optional) The type of learning rate decay (default: None). See:
  #  * https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate
  #  * opennmt/utils/decay.py
  # This value may change the semantics of other decay options. See the documentation or the code.
  decay_type: NoamDecay
  decay_params:
    model_dim: 512
    warmup_steps: 4000
  # (optional unless decay_type is set) The learning rate decay rate.
  decay_rate: 0.05
  # (optional unless decay_type is set) Decay every this many steps.
  decay_steps: 2000
  # (optional) If true, the learning rate is decayed in a staircase fashion (default: True).
  staircase: true
  # (optional) After how many steps to start the decay (default: 0).
  start_decay_steps: 1000
  # (optional) Stop decay when this learning rate value is reached (default: 0).
  minimum_learning_rate: 0.00001
  # (optional) Type of scheduled sampling (can be "constant", "linear", "exponential",
  # or "inverse_sigmoid", default: "constant").
  scheduled_sampling_type: inverse_sigmoid

# Training options.
train:
  # adjust the batch size to the amount of memory available to the GPU
  batch_size: 32
  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 20000
  # (optional) Save summaries every this many steps.
  save_summary_steps: 2000
  # (optional) Train for this many steps. If not set, train forever.
  max_steps: 40000